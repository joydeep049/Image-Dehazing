# -*- coding: utf-8 -*-
"""Network_Model_Residual.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19cUFBL0OzBXAzhUBlaKrvAJf9K8yR5GE

**Downloading Dataset**
"""

# !wget -O data.mat http://horatio.cs.nyu.edu/mit/silberman/nyu_depth_v2/nyu_depth_v2_labeled.mat

"""**Importing Libraries**"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import h5py
import math

from keras.models import Model
from keras.layers import Input, Activation, BatchNormalization, Conv2D, Conv3D
from keras.layers import Lambda, Concatenate, MaxPooling2D, Maximum, Add
from keras.initializers import RandomNormal
from keras.optimizers import SGD
from keras.losses import MeanSquaredError
from keras.callbacks import Callback,LearningRateScheduler
from keras.utils import plot_model

import keras.backend as K
K.set_image_data_format('channels_last')

import matplotlib.pyplot as plt
from matplotlib.pyplot import imshow

# %matplotlib inline

"""**util functions**"""

# Load Dataset from Google Drive
def load_train_dataset():
    """
    Load training dataset

    Returns:
    clean_image -- Clean image patches
    haze_image -- Hazy image patches
    transmission_value -- Transmission value which convert clean image to
                          haze image
    """

    file = 'data.mat'
    train_dataset = h5py.File(file, 'r')
    clean_image = np.array(train_dataset['clear_image'][:])
    haze_image = np.array(train_dataset['haze_image'][:])
    transmission_map = np.array(train_dataset['transmission_map'])
    transmission_map_refine = np.array(train_dataset['transmission_map_refine'])

    return clean_image, haze_image, transmission_map, transmission_map_refine


# Gaussian Weight Initializtion for layers
weight_init = RandomNormal(mean=0.0, stddev=0.001)


# LearningRate Decay function
def lr_schedule(epoch,lr, logs={}):
    """
    Learning Rate Deacy scheduler

    Arguments:
    epoch -- current epoch number
    lr -- current learning rate
    log -- dictionary storing the logs of training

    Returns:
    lr -- learning rate for next epoch
    """

    print('learning_rate:',lr)
    logs.update({'lr': lr})
    if epoch in (49,99):
        return lr*0.5
    else:
        return lr

"""**Preparing Train Dataset**"""

clean_image, haze_image, transmission_map, transmission_map_refine = load_train_dataset()

print ("Number of training examples:", clean_image.shape[0])
print ("Clean Image Patch shape:", clean_image.shape)
print ("Haze Image Patch shape:", haze_image.shape)
print ("Transmission Map shape:", haze_image.shape)
print ("Transmission Map Refine shape:", haze_image.shape)

residual_input = np.clip(((haze_image/255.0)/np.expand_dims(transmission_map_refine,axis=3)),0,1)
residual_output = np.clip((residual_input-clean_image),0,1)

"""#### Residual Model

**Designing**
"""

def ResidualBlock(X, iter):
    """
    Implementation of the single block of RNN

    Arguments:
    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)
    iter -- integer, used to name layers, depending on current residual block

    Returns:
    X -- output of the identity block, tensor of shape (n_H, n_W, n_C)
    """

    # Save the input value
    X_shortcut = X

    # BATCHNORMALIZATION -> CONV Block
    X = BatchNormalization(axis = 3, name = 'res_batchnorm_' + str(iter))(X)
    X = Conv2D(1, (3, 3), strides = (1,1), padding = 'same', kernel_initializer = weight_init, name = 'res_conv_' + str(iter))(X)

    # Add shortcut value to main path, and pass it through a RELU activation
    X = Add(name = 'res_add_'+ str(iter))([X,X_shortcut])
    X = Activation('relu', name = 'res_activation_'+ str(iter))(X)

    return X

def ResidualModel(input_shape):
    """
    Implementation of the Model.

    Arguments:
    input_shape -- shape of the images of the dataset
                   (height, width, channels) as a tuple.

    Returns:
    model -- a Model() instance in Keras
    """

    X_input = Input(input_shape, name = 'input1')

    # CONV -> RELU Block applied to X
    X = Conv2D(16, (3, 3), strides = (1, 1), padding = 'same', kernel_initializer = weight_init, name = 'conv1')(X_input)
    X = Activation('relu', name = 'activation1')(X)

    # X = Conv2D(8, (1, 1), kernel_initializer = weight_init, name='test_conv')(X)

    for i in range(17):
        X = ResidualBlock(X, i)

    # CONV BLock
    X = Conv2D(3, (3, 3), strides = (1, 1), padding = 'same', kernel_initializer = weight_init, name = 'conv2')(X)
    X = Activation('relu', name = 'activation2')(X)

    # Create Keras model instance
    model = Model(inputs = X_input, outputs = X, name='TransmissionModel')

    return model

"""**Defining Constants**"""

vertices = tf.constant([
    [0.0, 0.0, 0.0],  # Vertex 0
    [1.0, 0.0, 0.0],  # Vertex 1
    [1.0, 1.0, 0.0],  # Vertex 2
    [0.0, 1.0, 0.0],  # Vertex 3
    [0.0, 0.0, 1.0],  # Vertex 4
    [1.0, 0.0, 1.0],  # Vertex 5
    [1.0, 1.0, 1.0],  # Vertex 6
    [0.0, 1.0, 1.0]   # Vertex 7
], dtype=tf.float32)

faces = tf.constant([
    [0, 1, 2], [0, 2, 3],  # Bottom face
    [4, 5, 6], [4, 6, 7],  # Top face
    [0, 1, 5], [0, 5, 4],  # Front face
    [2, 3, 7], [2, 7, 6],  # Back face
    [1, 2, 6], [1, 6, 5],  # Right face
    [0, 3, 7], [0, 7, 4]   # Left face
], dtype=tf.int32)

def compute_laplacian(vertices, faces):
    """
    Computes the Laplacian for each vertex in the mesh.

    Args:
        vertices: A Tensor of shape (N, 3), where N is the number of vertices.
        faces: A Tensor of shape (M, 3), where M is the number of faces (triangles).

    Returns:
        laplacian: A Tensor of shape (N, 3) representing the Laplacian at each vertex.
    """
    N = tf.shape(vertices)[0]
    laplacian = tf.zeros_like(vertices)

    for i in range(tf.shape(faces)[0]):
        for j in range(3):
            v0 = faces[i, j]
            v1 = faces[i, (j + 1) % 3]
            v2 = faces[i, (j + 2) % 3]

            laplacian_v0 = (vertices[v1] - vertices[v0]) + (vertices[v2] - vertices[v0])
            laplacian = tf.tensor_scatter_nd_add(laplacian, [[v0]], [laplacian_v0])

    laplacian /= 2.0

    return laplacian

def higher_order_srl_loss(vertices, faces):
    """
    Computes the higher-order surface regularization loss.

    Args:
        vertices: A Tensor of shape (N, 3), where N is the number of vertices.
        faces: A Tensor of shape (M, 3), where M is the number of faces (triangles).

    Returns:
        loss: A scalar Tensor representing the surface regularization loss.
    """
    laplacian = compute_laplacian(vertices, faces)
    loss = tf.reduce_mean(tf.reduce_sum(tf.square(laplacian), axis=1))
    return loss

def custom_loss(y_true, y_pred, vertices, faces):
    mse_loss = tf.keras.losses.MeanSquaredError()(y_true, y_pred)
    # Calculate higher-order surface regularization loss
    srl_loss = higher_order_srl_loss(vertices, faces)
    # Combine losses (you can adjust the weighting factor)
    total_loss = mse_loss + srl_loss
    return total_loss

"""**Compiling**"""

model2 = ResidualModel(residual_input.shape[1:])
model2.summary()
model2.compile(optimizer=SGD(0.001), loss=lambda y_true, y_pred: custom_loss(y_true, y_pred, vertices, faces))
# model2.compile(optimizer=SGD(0.001), loss = MeanSquaredError())

"""**Training**"""

history2 = model2.fit(residual_input, residual_output, batch_size = 30, epochs = 150, callbacks=[LearningRateScheduler(lr_schedule)])

"""**Generating Models and Graph**"""

plot_model(model2, 'res_model.png')
plot_model(model2, 'res_model_shape.png', True)

plt.plot(history2.history['lr'])
plt.title('model learning rate')
plt.ylabel('learning rate')
plt.xlabel('epoch')
plt.legend(['train'], loc='upper right')
plt.savefig('res150-30-lr.png')
plt.show()
plt.plot(history2.history['loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train'], loc='upper right')
plt.savefig('res150-30-loss.png')
plt.show()

# model2.save('resmodel_150_30.h5')
# model2.save_weights('resmodel_150_30_weights.h5')